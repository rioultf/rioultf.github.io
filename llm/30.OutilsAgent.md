---
author:
- Fran√ßois Rioult
lang: fr
title: Application des LLM
subtitle: Outils - Agent
---

Un agent regroupe trois composants :

* un LLM
* des outils, choisis par le LLM
* un framework, qui ex√©cute l'outil


# Model context protocol

Pour interfacer un LLM avec un serveur externe faisant office de source de connaissance, on utilise le principe des outils : le LLM est interrog√© avec le prompt utilisateur accompagn√© des outils et indique si un appel d'outil est n√©cessaire. On lui soumet √† nouveau le prompt avec les donn√©es retourn√©es par l'outil.

La conception d'un serveur externe doit √™tre simple, le principe est d'interfacer √† moindre frais un service local existant (API, base de donn√©es, calculateur). De plus, le serveur doit √™tre capable de fournir la liste des outils qu'il propose (`Processing request of type ListToolsRequest`).

MCP est une sp√©cification de serveurs g√©n√©ralistes permettant d'interfacer une API avec les mod√®les d'Anthropic. Anthropic fournit un environnement de d√©veloppement complet de serveurs MCP, avec des SDK dans tous les langages. 

Les moyens de communication sont frustres - deux protocoles de transport :

* STDIO : par entr√©e/sortie standard, pour encapsuler des scripts de ligne de commande.
* SSE : Server-Sent Events est une technologie de push pour envoyer des notifications √† un client via HTTP.

Le d√©veloppement d'un serveur consiste en l'√©criture de code, par ex. Python + annotations. La librairie fournit un inspecteur du serveur :

    # active le service
    uvx mcp run server.py 
    # active et inspecte
    uvx mcp dev server.py 

```bash
# installation uv
curl -LsSf https://astral.sh/uv/install.sh | sh
uv init mcp-server-demo
cd mcp-server-demo/
uv add "mcp[cli]"
uvx run mcp  # √©chec
source .venv/bin/activate
uvx run mcp  # √©chec
mcp dev server.py 
Need to install the following packages:
@modelcontextprotocol/inspector@0.9.0
Ok to proceed? (y) y

Starting MCP inspector...
‚öôÔ∏è Proxy server listening on port 6277
üîç MCP Inspector is up and running at http://127.0.0.1:6274 üöÄ

```

# OpenWebUi

`open-webui` ne propose pas l'int√©gration de MCP. √Ä la place, il propose `mcpo`, un wrapper de MCP qui fournit une interface OpenAPI √† un exc√©cutable de serveur MCP. `mcpo` peut lancer plusieurs MCP :

    uvx mcpo --config /path/to/config.json

Ci-dessous une pile test√©e avec `open-webui` :

* le serveur de temps issu du [catalogue](https://github.com/modelcontextprotocol/servers)
* un gestionnaire de graphe
* des tests du [quickStart](https://github.com/modelcontextprotocol/python-sdk?tab=readme-ov-file#quickstart)

```json
{
    "mcpServers": {
        "time": {
            "command": "uvx",
            "args": [
                "mcp-server-time",
                "--local-timezone=America/New_York"
            ]
        },
        "memory": {
            "command": "npx",
            "args": [
                "-y",
                "@modelcontextprotocol/server-memory"
            ]
        },
        "mcp": {
            "command": "mcp",
            "args": [
                "run",
                "mcp-server-demo.py"
            ]
        }
    }
}
```

Chaque outil aura sa propre route :

    http://localhost:8000/memory
    http://localhost:8000/time
    http://localhost:8000/mcp


## Mise au point d'un serveur MCP avec `open-webui`

1. d√©velopper le MCP et tester la correction du code avec :

    mcp dev mcp-server-demo.py

1. exposer √† l'aide de `mcpo`


# PocketFlow

* [design pattern d'agent](https://zacharyhuang.substack.com/p/llm-agent-internal-as-a-graph-tutorial)


[Guide de conception d'un agent](https://github.com/The-Pocket/PocketFlow/blob/main/docs/guide.md)

[Cookbook flow](https://github.com/The-Pocket/PocketFlow/blob/main/cookbook/pocketflow_demo.ipynb)

Why not built-in?: I believe it's a bad practice for vendor-specific APIs in a general framework:

    API Volatility: Frequent changes lead to heavy maintenance for hardcoded APIs.
    Flexibility: You may want to switch vendors, use fine-tuned models, or run them locally.
    Optimizations: Prompt caching, batching, and streaming are easier without vendor lock-in.


`pocketFlow`est organis√© en 

* noeuds : pre, exec, post. Retourne une action
* flow : encha√Ænement/organisation de noeud, en fonction des actions
  * transition basique

        node_a >> node_b : 
        if node_a.post() returns "default", go to node_b. (Equivalent to node_a - "default" >> node_b)

    * transition par action nomm√©e : 
    
        node_a - "action_name" >> node_b This means if node_a.post() returns "action_name", go to node_b.


* communications : 
    * share store
    * environnement pour batch, immutable

```python
# Define the flow connections
review - "approved" >> payment        # If approved, process payment
review - "needs_revision" >> revise   # If needs changes, go to revision
review - "rejected" >> finish         # If rejected, finish the process

revise >> review   # After revision, go back for another review
payment >> finish  # After payment, finish the process

flow = Flow(start=review)
```

On peut imbriquer les *flow* :

```python
# Create a sub-flow
node_a >> node_b
subflow = Flow(start=node_a)

# Connect it to another node
subflow >> node_c

# Create the parent flow
parent_flow = Flow(start=subflow)
```

<!------------------------------------------------------------>



# Divers

* Cloud based (un-quantized) models are typically dramatically better at following instructions and forming valid JSON matching the required tool-call.
* un mod√®le qui fonctionne bien sur les appels d'outil : https://ollama.com/library/mistral-small3.1:24b-instruct-2503-fp16
* pour le monitoring des services, on peut utiliser [uptime-kuma](https://github.com/louislam/uptime-kuma)
* <https://github.com/TheR1D/shell_gpt>
Le choix de l'action et des param√®tres est effectu√© par le mod√®le √† partir du prompt.

Test√© sur `smollm2:1.7b` avec `ollama` :

```python
import ollama

response = ollama.chat(
    model='smollm2:1.7b',
    messages=[{'role': 'user', 'content':
        'What is the weather in Toronto?'}],

		# provide a weather checking tool to the model
    tools=[{
      'type': 'function',
      'function': {
        'name': 'get_current_weather',
        'description': 'Get the current weather for a city',
        'parameters': {
          'type': 'object',
          'properties': {
            'city': {
              'type': 'string',
              'description': 'The name of the city',
            },
          },
          'required': ['city'],
        },
      },
    },
  ],
)

print(response['message']['tool_calls'])
```
