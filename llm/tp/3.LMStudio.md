---
author:
- François Rioult
lang: fr
title: Application des LLM
subtitle: Registre : LMStudio
---

LMStudio est un *registre* : il sert de l'inférence à partir de modèle locaux.

* télécharge un modèle GGUF
* chaque modèle peut être paramétré :

  * load

    * taille du contexte
    * GPU / CPU
    * seed

  * prompt
    
    * *jinja* template
    * system prompt

  * inférence 

    * température 
    * sortie structurée (JSON)
    * top K
    * min P
    * top P

* permet d'instancier un *serveur* pour l'inférence (Developer -> Server Settings)

  * serveur local : `LMStudio` propose une API `chat_completions` et peut être utilisé par AnythingLLM
  * accepte les requêtes MCP 
  * positionne CORS (pour pouvoir assembler la requête dans un navigateur)

* la section `Developer` permet d'interagir avec les modèles (élucider si le paramétrage surcharge celui du modèle):

  * liste des end-points supportés 

  * onglets
    
    *information (quantisation, architecture, taille)
    * system prompt 
    * inférence 

        * température 
        * sortie structurée (JSON)
        * top K
        * min P
        * top P

  * paramétrer le système de log


# Installation

* créer un dossier `~/software` destiné à stocker les logiciels utilisés dans ce TP
* y télécharger l'[AppImage](https://lmstudio.ai/download/latest/linux/x64)

        chmod +x LM-Studio-0.3.35-1-x64.AppImage 
        ./LM-Studio-0.3.35-1-x64.AppImage
        LM-Studio-0.3.17-11-x64.AppImage --no-sandbox

* on installe un premier modèle (onglet vertical)

  * rechercher https://model.lmstudio.ai/download/lmstudio-community/SmolLM2-1.7B-Instruct-GGUF

  
Les modèles sont stockés au format GGUF dans `ˇ/.lmstudio/models`.

# Exercices

1. effectuer des requêtes sur `smollm2-1.7b-instruct:2`

                hi

1. analyser les statistiques (survol de l'ampoule) :
```
"stats": {
    "stopReason": "eosFound",
    "tokensPerSecond": 2.039456782601547,
    "numGpuLayers": -1,
    "timeToFirstTokenSec": 15.076,
    "totalTimeSec": 4.903,
    "promptTokensCount": 31,
    "predictedTokensCount": 10,
    "totalTokensCount": 41
  }
```

1. découvrir les modes d'interaction avec le chatbot

  * regénérer la réponse
  * continuer la discussion
  * brancher

1. dans `Developper`, découvrir la requête `cURL` à exécuter : 

```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "smollm2-1.7b-instruct",
    "messages": [
        {
            "role": "system",
            "content": "Always answer in rhymes. Today is Thursday"
        },
        {
            "role": "user",
            "content": "What day is it today?"
        }
    ],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
}'
```

```json
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "smollm2-1.7b-instruct",
    "messages": [
        {
            "role": "system",
            "content": "Always answer in rhymes. Today is Thursday"
        },
        {
            "role": "user",
            "content": "What day is it today?"
        }
    ],
    "temperature": 0.7,
    "max_tokens": -1,
    "stream": false
}'
```

1. [script d'interrogation de LMStudio](script/chatCompletions-LMS.sh), basé sur [le script d'interrogation d'OpenRouter](script/chatCompletions.sh)

1. mettre en évidence la limite de la fenêtre de contexte :

    pip install tiktoken --user
    python3 comptine.py 100 > comptine.txt 2> comptine_tokens.txt

    Répète uniquement les informations des vers 1 à 30 dans l’ordre.

  1. expérience 0 : zero-shot

    réussit bien à 
    
```
Vers 1: Le nom de carnet 1 est Marin.
...
Vers 69: Le nom de carnet 69 est Marin.
Refrain: Récapitule tous les carnets vus jusqu’ici.

Répète uniquement les informations des vers 1 à 30 dans l’ordre.
```

  1. expérience 1 : one-shot ?

    * indication des 100 premiers

      * à 14 : `Marin` devient `marin`
      * à 28, apparition de `mari`
      * répète tout le carnet
      * ne répond pas à la question

* avec 160 vers, ce qui semble être la limite en tokens pour ce modèle (le calcul diffère de celui fourni par tiktoken), la réponse est :

```
Les nom de la premiere carte est Marin, la second carte Marina, la troisième carte Marinne et le quart carte Marinier. Les noms des deuxièmes cartes sont Marinelle, Marinien et Marinneur. La quatrième carte est Marinier et le fifth carte est mariner. L'sixième carte est mariniard, le septième carte est marinier, le huitième carte est mariner, le neuvième carte est marinier, le onzièm carte est mariner et la dizaine carte est marin.
```

# Limites

* comment tracer précisément ce qui rentre dans LMStudio en terme d'API